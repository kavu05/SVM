{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    data = []\n",
    "    with open(file_name, errors='ignore') as file:\n",
    "        data = file.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv('agr_en_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_SVM(data):\n",
    "    label = []\n",
    "    Text = []\n",
    "    for text in data:\n",
    "        label.append(text[0])\n",
    "        Text.append(text[2:])\n",
    "    return (label,Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestAndTrainPortion(x, split_len, data):\n",
    "    \n",
    "    #if this is the 1st fold, split the dataset into test and training set as follows\n",
    "    if x == 1:\n",
    "        #the data in positions 0-split_len will be the test set\n",
    "        test_set = data[:split_len]\n",
    "        #the data in positions from split_len until the end will be the test set\n",
    "        train_set = data[split_len:]\n",
    "    elif x == 10:\n",
    "        #similarly, if it is the10th fold, we need to split the data after the 9th fold. \n",
    "        test_set = data[(split_len*(x-1)):]\n",
    "        train_set = data[:(split_len*(x-1))]\n",
    "    else:\n",
    "        #if it is a fold between 2nd-9th, we will need to take the data between two data points as follows.\n",
    "        test_set = data[(split_len*(x-1)):(split_len*x)]\n",
    "        train_set= data[:(split_len*(x-1))]\n",
    "        train_set.extend(data[(split_len*x):])\n",
    "        \n",
    "    return test_set, train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 \n",
    "\n",
    "#calculate the length of each fold by dividing the size of the dataset with the number of k\n",
    "split_len = int(len(data)/k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    x=i+1#i is 0-9, need 1-10 for functions\n",
    "\n",
    "    #create different test & train portions for each loop in 10-fold\n",
    "    test_set, train_set = createTestAndTrainPortion(x, split_len,data)\n",
    "    \n",
    "    \n",
    "    train_labels, train_Text = preprocess_data_SVM(train_set)\n",
    "    test_labels, test_Text = preprocess_data_SVM(test_set)\n",
    "\n",
    "    #Feature Extraction/Vectorisation\n",
    "    vectorizer = TfidfVectorizer(min_df=1,sublinear_tf=True)\n",
    "    \n",
    "    train_vectors = vectorizer.fit_transform(train_Text)\n",
    "    test_vectors = vectorizer.transform(test_Text)\n",
    "    \n",
    "    #SVM Linear Kernel \n",
    "    classifier = svm.SVC(kernel = 'linear')\n",
    "    classifier.fit(train_vectors, train_labels)#train classifier on train data\n",
    "    prediction = classifier.predict(test_vectors)#predict test data\n",
    "    \n",
    "    #get classification results/evaluate model\n",
    "    report = metrics.precision_recall_fscore_support(test_labels, prediction)\n",
    "    report_accuracy = accuracy_score(test_labels, prediction)\n",
    "\n",
    "    #save tuple results to lists to later get 10-fold average\n",
    "    precision.append(report[0])\n",
    "    recall.append(report[1])\n",
    "    f1.append(report[2])\n",
    "    accuracy.append(report_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageResults(report_list):\n",
    "    \n",
    "    total_cag = 0\n",
    "    total_nag = 0\n",
    "    total_oag = 0\n",
    "\n",
    "    for item in report_list:\n",
    "        total_cag += item[0]\n",
    "        total_nag += item[1]\n",
    "        total_oag += item[2]\n",
    "\n",
    "    av_cag = total_cag/len(report_list)\n",
    "    av_nag = total_nag/len(report_list)\n",
    "    av_oag = total_oag/len(report_list)\n",
    "    \n",
    "    \n",
    "    return (av_cag + av_nag + av_oag)/3\n",
    "\n",
    "#There is only one score per fold for all the three labels regarding accuracy \n",
    "def getAccuracyAverage(report_list):\n",
    "    total = 0\n",
    "\n",
    "    for item in report_list:\n",
    "        total += item\n",
    "\n",
    "    score = total/len(report_list)\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.5848482184532858 \n",
      "Recall:  0.5450336657929572 \n",
      "F-score:  0.5253640150052398\n",
      "Accuracy:  0.8335845862090636\n"
     ]
    }
   ],
   "source": [
    "precision_result = getAverageResults(precision)\n",
    "recall_result = getAverageResults(recall)\n",
    "f_score_result = getAverageResults(f1)\n",
    "accuracy_result = getAccuracyAverage(accuracy)\n",
    "print(\"Precision: \", precision_result, \"\\nRecall: \", recall_result, \"\\nF-score: \", f_score_result)\n",
    "print(\"Accuracy: \", accuracy_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
